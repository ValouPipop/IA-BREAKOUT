"""
Comparaison des deux modes de mise Ã  jour :
  - Mode "EPISODE" : 1 update Ã  la fin de chaque partie
  - Mode "LIFE"    : 1 update Ã  chaque perte de vie

EntraÃ®ne les deux modÃ¨les sÃ©quentiellement puis affiche les courbes.
â˜… Version amÃ©liorÃ©e avec suivi dÃ©taillÃ© de toutes les mÃ©triques.
"""

import gymnasium as gym
import ale_py
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from collections import deque
import time

from CA2C import (
    A2CNet, FrameStack, compute_returns, update_model,
    GAME, K_FRAMES, GAMMA, LR, CV, ENTROPY_COEF, GRAD_CLIP, DEVICE
)

# â”€â”€ Enregistrement des environnements Atari â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
gym.register_envs(ale_py)

# â”€â”€ ParamÃ¨tres de comparaison â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
N_EPISODES  = 5000       # Nombre d'Ã©pisodes par mode
PRINT_EVERY = 50         # Affichage tous les N Ã©pisodes


def train_with_mode(mode: str):
    """
    EntraÃ®ne un modÃ¨le A2C avec le mode spÃ©cifiÃ©.
    mode : "EPISODE" ou "LIFE"
    Retourne un dict avec toutes les mÃ©triques collectÃ©es.
    """
    assert mode in ("EPISODE", "LIFE"), f"Mode inconnu : {mode}"

    print(f"\n{'='*80}")
    print(f"  ENTRAÃŽNEMENT MODE : {mode}")
    print(f"{'='*80}\n")

    env       = gym.make(GAME)
    n_actions = env.action_space.n
    stacker   = FrameStack(K_FRAMES)

    model     = A2CNet(n_actions, K_FRAMES).to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LR)
    scheduler = optim.lr_scheduler.LinearLR(
        optimizer,
        start_factor=1.0,
        end_factor=0.1,
        total_iters=N_EPISODES
    )

    # â”€â”€ MÃ©triques Ã  collecter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    episode_rewards = []       # RÃ©compense brute par Ã©pisode
    moving_avg      = []       # Moyenne glissante (100 Ã©pisodes)
    all_losses      = []       # Loss totale moyenne par Ã©pisode
    all_actor_losses  = []     # Loss acteur moyenne par Ã©pisode
    all_critic_losses = []     # Loss critique moyenne par Ã©pisode
    all_entropies   = []       # Entropie moyenne par Ã©pisode
    all_steps       = []       # Nombre de steps par Ã©pisode
    all_updates     = []       # Nombre d'updates par Ã©pisode
    all_seg_lens    = []       # Longueur moyenne des segments par Ã©pisode
    all_lrs         = []       # Learning rate par Ã©pisode
    all_max_scores  = []       # Meilleur score atteint jusque-lÃ 

    window    = deque(maxlen=100)
    best_avg  = 0.0
    max_score = 0.0
    start_time = time.time()

    # â”€â”€ GPU memory tracking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    gpu_available = DEVICE.type == "cuda"

    for ep in range(1, N_EPISODES + 1):

        obs, info = env.reset()
        state     = stacker.reset(obs)

        # Fire-on-reset
        obs, _, terminated, truncated, info = env.step(1)
        if not (terminated or truncated):
            state = stacker.step(obs)

        lives = info.get("lives", 5)

        seg_states  = []
        seg_actions = []
        seg_rewards = []
        seg_dones   = []

        ep_reward    = 0.0
        done         = False
        ep_steps     = 0
        n_updates    = 0

        # MÃ©triques d'update pour cet Ã©pisode
        ep_losses       = []
        ep_actor_losses = []
        ep_critic_losses = []
        ep_entropies    = []
        ep_seg_lens     = []

        while not done:
            state_t = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)

            with torch.no_grad():
                logits, _ = model(state_t)

            dist   = torch.distributions.Categorical(logits=logits)
            action = dist.sample().item()

            next_obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated

            new_lives = info.get("lives", lives)
            life_lost = (new_lives < lives)
            lives     = new_lives

            clipped_reward = np.clip(reward, -1.0, 1.0)
            effective_done = float(done or life_lost)

            next_state = stacker.step(next_obs)

            seg_states.append(state)
            seg_actions.append(action)
            seg_rewards.append(clipped_reward)
            seg_dones.append(effective_done)

            ep_reward += reward
            ep_steps  += 1
            state      = next_state

            # â”€â”€ Mode LIFE : update Ã  chaque perte de vie â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if mode == "LIFE" and life_lost and not done:
                result = update_model(
                    model, optimizer,
                    seg_states, seg_actions, seg_rewards, seg_dones
                )
                if result is not None:
                    ep_losses.append(result["loss"])
                    ep_actor_losses.append(result["L_actor"])
                    ep_critic_losses.append(result["L_critic"])
                    ep_entropies.append(result["entropy"])
                    ep_seg_lens.append(result["seg_len"])
                    n_updates += 1
                seg_states  = []
                seg_actions = []
                seg_rewards = []
                seg_dones   = []

            # Fire aprÃ¨s perte de vie
            if life_lost and not done:
                obs, _, terminated, truncated, info = env.step(1)
                if not (terminated or truncated):
                    state = stacker.step(obs)
                else:
                    done = True

        # â”€â”€ Update sur le segment restant (fin d'Ã©pisode) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if len(seg_rewards) >= 2:
            result = update_model(
                model, optimizer,
                seg_states, seg_actions, seg_rewards, seg_dones
            )
            if result is not None:
                ep_losses.append(result["loss"])
                ep_actor_losses.append(result["L_actor"])
                ep_critic_losses.append(result["L_critic"])
                ep_entropies.append(result["entropy"])
                ep_seg_lens.append(result["seg_len"])
                n_updates += 1

        scheduler.step()

        # â”€â”€ Collecte des mÃ©triques â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        episode_rewards.append(ep_reward)
        window.append(ep_reward)
        avg = np.mean(window)
        moving_avg.append(avg)

        max_score = max(max_score, ep_reward)
        all_max_scores.append(max_score)

        if avg > best_avg:
            best_avg = avg
            torch.save(model.state_dict(), f"a2c_best_{mode.lower()}.pth")

        # Moyennes des mÃ©triques de cet Ã©pisode
        all_losses.append(np.mean(ep_losses) if ep_losses else 0.0)
        all_actor_losses.append(np.mean(ep_actor_losses) if ep_actor_losses else 0.0)
        all_critic_losses.append(np.mean(ep_critic_losses) if ep_critic_losses else 0.0)
        all_entropies.append(np.mean(ep_entropies) if ep_entropies else 0.0)
        all_steps.append(ep_steps)
        all_updates.append(n_updates)
        all_seg_lens.append(np.mean(ep_seg_lens) if ep_seg_lens else 0.0)
        all_lrs.append(optimizer.param_groups[0]['lr'])

        # â”€â”€ Affichage dÃ©taillÃ© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if ep % PRINT_EVERY == 0:
            elapsed     = time.time() - start_time
            eps_per_sec = ep / elapsed
            eta         = (N_EPISODES - ep) / eps_per_sec

            # GPU memory
            gpu_info = ""
            if gpu_available:
                gpu_alloc = torch.cuda.memory_allocated() / 1024**3
                gpu_reserv = torch.cuda.memory_reserved() / 1024**3
                gpu_info = f" | GPU: {gpu_alloc:.2f}/{gpu_reserv:.2f} Go"

            current_lr = optimizer.param_groups[0]['lr']
            avg_steps  = np.mean(all_steps[-PRINT_EVERY:])
            avg_upd    = np.mean(all_updates[-PRINT_EVERY:])
            avg_seg    = np.mean(all_seg_lens[-PRINT_EVERY:])
            avg_loss   = np.mean(all_losses[-PRINT_EVERY:]) if any(all_losses[-PRINT_EVERY:]) else 0.0
            avg_la     = np.mean(all_actor_losses[-PRINT_EVERY:]) if any(all_actor_losses[-PRINT_EVERY:]) else 0.0
            avg_lc     = np.mean(all_critic_losses[-PRINT_EVERY:]) if any(all_critic_losses[-PRINT_EVERY:]) else 0.0
            avg_ent    = np.mean(all_entropies[-PRINT_EVERY:]) if any(all_entropies[-PRINT_EVERY:]) else 0.0

            print(f"  [{mode:7s}] Ã‰p {ep:4d}/{N_EPISODES}"
                  f" | RÃ©c: {ep_reward:5.1f}"
                  f" | Moy100: {avg:6.2f}"
                  f" | Best: {best_avg:6.2f}"
                  f" | Max: {max_score:5.0f}")
            print(f"           "
                  f" | Steps: {avg_steps:5.0f}"
                  f" | Upd: {avg_upd:3.1f}"
                  f" | SegLen: {avg_seg:5.0f}"
                  f" | LR: {current_lr:.2e}"
                  f" | ETA: {eta/60:.0f}min")
            print(f"           "
                  f" | Loss: {avg_loss:7.3f}"
                  f" | L_act: {avg_la:7.3f}"
                  f" | L_crit: {avg_lc:7.3f}"
                  f" | H: {avg_ent:.3f}"
                  f"{gpu_info}")
            print()

    env.close()

    elapsed_total = time.time() - start_time
    print(f"\n  [{mode}] TerminÃ© en {elapsed_total/60:.1f} minutes")
    print(f"  [{mode}] Meilleure moyenne (100 Ã©p.) : {best_avg:.2f}")
    print(f"  [{mode}] Score max atteint : {max_score:.0f}")
    print(f"  [{mode}] Steps moyen/Ã©pisode : {np.mean(all_steps):.0f}")
    print(f"  [{mode}] Updates moyen/Ã©pisode : {np.mean(all_updates):.1f}")

    torch.save(model.state_dict(), f"a2c_final_{mode.lower()}.pth")

    return {
        "rewards": episode_rewards,
        "moving_avg": moving_avg,
        "losses": all_losses,
        "actor_losses": all_actor_losses,
        "critic_losses": all_critic_losses,
        "entropies": all_entropies,
        "steps": all_steps,
        "updates": all_updates,
        "seg_lens": all_seg_lens,
        "lrs": all_lrs,
        "max_scores": all_max_scores,
        "best_avg": best_avg,
        "max_score": max_score,
        "total_time": elapsed_total,
    }


def smooth(data, window=100):
    """Moyenne glissante pour lisser les courbes."""
    if len(data) < window:
        return data
    return np.convolve(data, np.ones(window)/window, mode='valid').tolist()


def plot_comparison(results: dict):
    """Affiche les courbes de comparaison dÃ©taillÃ©es."""

    fig, axes = plt.subplots(3, 2, figsize=(20, 16))
    fig.suptitle("Comparaison LIFE vs EPISODE â€“ MÃ©triques dÃ©taillÃ©es", fontsize=16, fontweight='bold')

    colors = {
        "EPISODE": {"raw": "lightcoral",   "avg": "red"},
        "LIFE":    {"raw": "lightskyblue", "avg": "dodgerblue"},
    }

    # â”€â”€ 1. RÃ©compenses â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ax = axes[0, 0]
    for mode, data in results.items():
        c = colors[mode]
        ax.plot(data["rewards"], alpha=0.1, color=c["raw"])
        ax.plot(data["moving_avg"], color=c["avg"], linewidth=2, label=f"{mode} (moy 100)")
    ax.set_xlabel("Ã‰pisode")
    ax.set_ylabel("RÃ©compense")
    ax.set_title("RÃ©compenses par Ã©pisode")
    ax.legend(fontsize=11)
    ax.grid(alpha=0.3)

    # â”€â”€ 2. Meilleur score atteint â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ax = axes[0, 1]
    for mode, data in results.items():
        c = colors[mode]
        ax.plot(data["max_scores"], color=c["avg"], linewidth=2, label=f"{mode} (max cumulÃ©)")
    ax.set_xlabel("Ã‰pisode")
    ax.set_ylabel("Score max atteint")
    ax.set_title("Progression du meilleur score")
    ax.legend(fontsize=11)
    ax.grid(alpha=0.3)

    # â”€â”€ 3. Loss totale â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ax = axes[1, 0]
    for mode, data in results.items():
        c = colors[mode]
        smoothed = smooth(data["losses"])
        ax.plot(smoothed, color=c["avg"], linewidth=1.5, label=f"{mode}", alpha=0.8)
    ax.set_xlabel("Ã‰pisode")
    ax.set_ylabel("Loss totale")
    ax.set_title("Loss totale (lissÃ©e sur 100 Ã©p.)")
    ax.legend(fontsize=11)
    ax.grid(alpha=0.3)

    # â”€â”€ 4. Entropie â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ax = axes[1, 1]
    for mode, data in results.items():
        c = colors[mode]
        smoothed = smooth(data["entropies"])
        ax.plot(smoothed, color=c["avg"], linewidth=1.5, label=f"{mode}", alpha=0.8)
    ax.set_xlabel("Ã‰pisode")
    ax.set_ylabel("Entropie")
    ax.set_title("Entropie de la politique (exploration)")
    ax.legend(fontsize=11)
    ax.grid(alpha=0.3)

    # â”€â”€ 5. Steps par Ã©pisode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ax = axes[2, 0]
    for mode, data in results.items():
        c = colors[mode]
        smoothed = smooth(data["steps"])
        ax.plot(smoothed, color=c["avg"], linewidth=1.5, label=f"{mode}", alpha=0.8)
    ax.set_xlabel("Ã‰pisode")
    ax.set_ylabel("Steps")
    ax.set_title("DurÃ©e des Ã©pisodes (steps)")
    ax.legend(fontsize=11)
    ax.grid(alpha=0.3)

    # â”€â”€ 6. Loss Actor vs Critic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ax = axes[2, 1]
    for mode, data in results.items():
        c = colors[mode]
        s_actor  = smooth(data["actor_losses"])
        s_critic = smooth(data["critic_losses"])
        ax.plot(s_actor, color=c["avg"], linewidth=1.5, linestyle='-',
                label=f"{mode} Actor", alpha=0.8)
        ax.plot(s_critic, color=c["avg"], linewidth=1.5, linestyle='--',
                label=f"{mode} Critic", alpha=0.5)
    ax.set_xlabel("Ã‰pisode")
    ax.set_ylabel("Loss")
    ax.set_title("DÃ©composition : Loss Actor (â€”) vs Critic (--)")
    ax.legend(fontsize=10)
    ax.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig("comparison_modes.png", dpi=150)
    print("\nâœ… Figure sauvegardÃ©e sous 'comparison_modes.png'")
    plt.show()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# POINT D'ENTRÃ‰E
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    print(f"Dispositif : {DEVICE}")
    print(f"Jeu : {GAME}")
    print(f"Ã‰pisodes par mode : {N_EPISODES}")
    print(f"Deux entraÃ®nements sÃ©quentiels : LIFE puis EPISODE")

    # â”€â”€ Infos GPU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if DEVICE.type == "cuda":
        print(f"GPU : {torch.cuda.get_device_name(0)}")
        print(f"VRAM totale : {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} Go")

    results = {}

    # â”€â”€ Premier entraÃ®nement : mode LIFE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if DEVICE.type == "cuda":
        torch.cuda.reset_peak_memory_stats()
    results["LIFE"] = train_with_mode("LIFE")

    # â”€â”€ LibÃ©rer la mÃ©moire GPU entre les deux entraÃ®nements â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if DEVICE.type == "cuda":
        torch.cuda.empty_cache()
        print(f"\n  ðŸ’¾ VRAM pic (LIFE) : {torch.cuda.max_memory_allocated()/1024**3:.2f} Go")
        torch.cuda.reset_peak_memory_stats()

    # â”€â”€ DeuxiÃ¨me entraÃ®nement : mode EPISODE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    results["EPISODE"] = train_with_mode("EPISODE")

    if DEVICE.type == "cuda":
        print(f"\n  ðŸ’¾ VRAM pic (EPISODE) : {torch.cuda.max_memory_allocated()/1024**3:.2f} Go")

    # â”€â”€ Comparaison graphique â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    plot_comparison(results)

    # â”€â”€ RÃ©sumÃ© final â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print(f"\n{'='*80}")
    print(f"  RÃ‰SUMÃ‰ FINAL")
    print(f"{'='*80}")
    print(f"  {'MÃ©trique':<30s} {'LIFE':>12s} {'EPISODE':>12s}")
    print(f"  {'-'*54}")

    life = results["LIFE"]
    ep   = results["EPISODE"]

    print(f"  {'Moy finale (100 Ã©p.)':<30s} {life['moving_avg'][-1]:>12.2f} {ep['moving_avg'][-1]:>12.2f}")
    print(f"  {'Meilleure moyenne':<30s} {life['best_avg']:>12.2f} {ep['best_avg']:>12.2f}")
    print(f"  {'Score max':<30s} {life['max_score']:>12.0f} {ep['max_score']:>12.0f}")
    print(f"  {'Steps moyen/Ã©pisode':<30s} {np.mean(life['steps']):>12.0f} {np.mean(ep['steps']):>12.0f}")
    print(f"  {'Updates moyen/Ã©pisode':<30s} {np.mean(life['updates']):>12.1f} {np.mean(ep['updates']):>12.1f}")
    print(f"  {'Seg. moyen (steps)':<30s} {np.mean(life['seg_lens']):>12.0f} {np.mean(ep['seg_lens']):>12.0f}")
    print(f"  {'Entropie finale':<30s} {life['entropies'][-1]:>12.3f} {ep['entropies'][-1]:>12.3f}")
    print(f"  {'Temps total':<30s} {life['total_time']/60:>11.1f}m {ep['total_time']/60:>11.1f}m")
    print(f"{'='*80}")
