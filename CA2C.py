"""
TP5 â€“ A2C Convolutionnel sur Atari (ALE/Breakout-v5)
======================================================
VERSION AMÃ‰LIORÃ‰E avec :
  1. Bonus d'entropie (exploration)
  2. Normalisation des avantages (stabilitÃ©)
  3. Reward clipping [-1, +1]
  4. Initialisation orthogonale des poids
  5. Fire-on-reset (Breakout nÃ©cessite l'action FIRE pour lancer la balle)
  6. Life-aware training (perte de vie = signal nÃ©gatif)
  7. Learning rate scheduler (decay progressif)
  8. â˜… Architecture IMPALA ResNet (extraction de features bien plus riche)

Mise Ã  jour des paramÃ¨tres : UNE SEULE FOIS Ã  la fin de chaque Ã©pisode.
Architecture CNN : IMPALA ResNet (Espeholt et al., 2018) â€“ bien plus profonde
que la Nature DQN, avec des blocs rÃ©siduels pour un meilleur flux de gradient.
"""

import gymnasium as gym
import ale_py
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
from collections import deque
import cv2

# â”€â”€ Enregistrement des environnements Atari â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
gym.register_envs(ale_py)

# â”€â”€ HyperparamÃ¨tres â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GAME         = "ALE/Breakout-v5"   # Jeu choisi
K_FRAMES     = 4                   # Nombre de frames empilÃ©es
GAMMA        = 0.99                # Facteur d'actualisation
LR           = 7e-4                # Taux d'apprentissage Adam (augmentÃ©)
CV           = 0.5                 # Coefficient perte critique
ENTROPY_COEF = 0.01                # â˜… NOUVEAU : coefficient d'entropie pour l'exploration
GRAD_CLIP    = 0.5                 # Seuil gradient clipping (Ï„)
N_EPISODES   = 15000               # EntraÃ®nement long pour convergence complÃ¨te
PRINT_EVERY  = 50                  # Affichage tous les N Ã©pisodes
SAVE_EVERY   = 1000                # â˜… Sauvegarde checkpoint tous les N Ã©pisodes
UPDATE_MODE  = "EPISODE"           # Mode EPISODE uniquement (LIFE ne converge pas)
DEVICE       = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"Dispositif utilisÃ© : {DEVICE}")
print(f"Jeu : {GAME}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1.  PRÃ‰TRAITEMENT VISUEL
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def preprocess_frame(frame: np.ndarray) -> np.ndarray:
    """
    RGB (210,160,3) â†’ Niveaux de gris â†’ RedimensionnÃ© 84Ã—84 â†’ NormalisÃ© [0,1].
    Retourne un tableau (84, 84) float32.
    """
    gray    = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)
    return resized.astype(np.float32) / 255.0


class FrameStack:
    """
    Maintient un buffer circulaire de K frames prÃ©traitÃ©es.
    L'Ã©tat st âˆˆ R^(KÃ—84Ã—84) est obtenu en empilant K frames consÃ©cutives.

    Pourquoi empiler K images au lieu d'une seule ?
    â†’ Une seule image est un "snapshot" statique. On ne peut PAS dÃ©duire
      la direction ni la vitesse de la balle/raquette. L'empilement de K=4
      frames permet au CNN de capturer le MOUVEMENT (diffÃ©rences inter-frames)
      et la VITESSE (amplitude des dÃ©placements entre frames).
    """
    def __init__(self, k: int = K_FRAMES):
        self.k      = k
        self.frames = deque(maxlen=k)

    def reset(self, frame: np.ndarray) -> np.ndarray:
        processed = preprocess_frame(frame)
        for _ in range(self.k):
            self.frames.append(processed)
        return self._get_state()

    def step(self, frame: np.ndarray) -> np.ndarray:
        self.frames.append(preprocess_frame(frame))
        return self._get_state()

    def _get_state(self) -> np.ndarray:
        return np.stack(list(self.frames), axis=0)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2.  ARCHITECTURE IMPALA ResNet (Espeholt et al., 2018)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ResidualBlock(nn.Module):
    """
    Bloc rÃ©siduel de l'architecture IMPALA :
      out = x + Conv(ReLU(Conv(ReLU(x))))

    Pourquoi les skip connections ?
    â†’ Dans un rÃ©seau profond, le gradient doit traverser TOUTES les couches
      pour mettre Ã  jour les premiers poids. Il peut devenir trÃ¨s petit
      (vanishing gradient). Le skip connection crÃ©e un "raccourci" direct :
      le gradient peut passer Ã  travers sans attÃ©nuation.
      RÃ©sultat : on peut empiler beaucoup plus de couches convolutionnelles
      sans perdre la capacitÃ© d'apprentissage.
    """
    def __init__(self, channels: int):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        residual = x
        out = torch.relu(x)
        out = self.conv1(out)
        out = torch.relu(out)
        out = self.conv2(out)
        return out + residual  # â† skip connection


class ConvSequence(nn.Module):
    """
    SÃ©quence convolutionnelle IMPALA :
      Conv2d(inâ†’out) â†’ MaxPool(3Ã—3, stride=2) â†’ ResBlock Ã— 2

    Chaque ConvSequence :
      - Augmente le nombre de canaux (features de plus en plus abstraites)
      - RÃ©duit la rÃ©solution spatiale de moitiÃ© (MaxPool)
      - Raffine les features via 2 blocs rÃ©siduels

    Comparaison avec Nature DQN :
      Nature DQN : 1 Conv par "Ã©tage" â†’ 3 couches total â†’ features basiques
      IMPALA     : 1 Conv + 2 ResBlocks par "Ã©tage" â†’ 5 couches par Ã©tage
                   Ã— 3 Ã©tages = 15 couches â†’ features BEAUCOUP plus riches
    """
    def __init__(self, in_channels: int, out_channels: int):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.res_block1 = ResidualBlock(out_channels)
        self.res_block2 = ResidualBlock(out_channels)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x = self.conv(x)
        x = self.maxpool(x)
        x = self.res_block1(x)
        x = self.res_block2(x)
        return x


class A2CNet(nn.Module):
    """
    RÃ©seau Actor-Critic avec backbone IMPALA ResNet :
      - Backbone CNN IMPALA : 3 ConvSequences (32â†’64â†’64 canaux)
        Chaque ConvSequence = Conv + MaxPool + 2 ResBlocks
        Total : 15 couches convolutionnelles (vs 3 pour Nature DQN)
      - TÃªte Acteur  : logits â†’ Ï€Î¸(a|st) = Softmax(gÎ¸(zt))
      - TÃªte Critique: scalaire â†’ VÏ•(st) = hÏ•(zt)

    â˜… Pourquoi IMPALA plutÃ´t que Nature DQN ?
      â†’ Le CNN Nature DQN (3 couches) extrait des features BASIQUES :
        bords, formes simples. C'est suffisant pour des jeux triviaux.
      â†’ IMPALA ResNet (15 couches + skip connections) extrait des
        features HIÃ‰RARCHIQUES : positions relatives, trajectoires,
        patterns de briques. Le rÃ©seau "comprend" mieux la scÃ¨ne.
      â†’ Les skip connections permettent au gradient de traverser
        les 15 couches sans s'Ã©vanouir â†’ convergence stable.
    """
    def __init__(self, n_actions: int, k_frames: int = K_FRAMES):
        super().__init__()

        # â”€â”€ Backbone CNN IMPALA ResNet â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #   3 Ã©tages de ConvSequence avec canaux croissants :
        #   (K_FRAMES, 84, 84) â†’ (32, 42, 42) â†’ (64, 21, 21) â†’ (64, 11, 11)
        channels = [32, 64, 64]
        self.conv_sequences = nn.ModuleList()
        in_ch = k_frames
        for out_ch in channels:
            self.conv_sequences.append(ConvSequence(in_ch, out_ch))
            in_ch = out_ch

        cnn_out_size = self._get_cnn_out(k_frames)

        # â”€â”€ Couche FC aprÃ¨s le CNN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #   IMPALA utilise 256 neurones (vs 512 pour Nature DQN) car le CNN
        #   extrait dÃ©jÃ  des features bien plus riches â†’ moins besoin de
        #   capacitÃ© dans la couche FC.
        self.fc = nn.Sequential(
            nn.Linear(cnn_out_size, 256),
            nn.ReLU(),
        )

        # â”€â”€ TÃªte Acteur â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.actor  = nn.Linear(256, n_actions)
        # â”€â”€ TÃªte Critique â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.critic = nn.Linear(256, 1)

        # â˜… Initialisation orthogonale des poids
        self._init_weights()

    def _init_weights(self):
        """
        Initialisation adaptÃ©e Ã  l'architecture IMPALA ResNet :

        1. Conv2d gÃ©nÃ©rales : orthogonale avec gain ReLU (âˆš2)
        2. Conv2 dans les ResidualBlocks : ZÃ‰RO (technique "Fixup")
           â†’ Chaque ResidualBlock commence comme une IDENTITÃ‰ :
             out = x + 0 = x
           Sans Ã§a, le signal est amplifiÃ© Ã  chaque bloc rÃ©siduel
           (Ã—âˆš2 par conv Ã— 6 blocs = explosion des activations â†’
           logits Ã©normes â†’ softmax dÃ©terministe â†’ entropie = 0)
        3. TÃªte Acteur : gain trÃ¨s faible (0.01)
           â†’ Logits initiaux â‰ˆ 0 â†’ Softmax â‰ˆ uniforme â†’ Entropie maximale
           â†’ L'agent EXPLORE avant de converger
        4. TÃªte Critique : gain trÃ¨s faible (0.01)
           â†’ Valeurs initiales â‰ˆ 0 (pas de biais initial)
        """
        for module in self.modules():
            if isinstance(module, nn.Conv2d):
                nn.init.orthogonal_(module.weight, gain=nn.init.calculate_gain('relu'))
                if module.bias is not None:
                    nn.init.zeros_(module.bias)
            elif isinstance(module, nn.Linear):
                nn.init.orthogonal_(module.weight, gain=1.0)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

        # â˜… Fixup : zero-init la 2Ã¨me conv de chaque ResidualBlock
        #   Cela rend chaque bloc rÃ©siduel = identitÃ© au dÃ©part.
        #   Le rÃ©seau apprend PROGRESSIVEMENT Ã  utiliser les rÃ©sidus.
        for conv_seq in self.conv_sequences:
            nn.init.zeros_(conv_seq.res_block1.conv2.weight)
            nn.init.zeros_(conv_seq.res_block1.conv2.bias)
            nn.init.zeros_(conv_seq.res_block2.conv2.weight)
            nn.init.zeros_(conv_seq.res_block2.conv2.bias)

        # TÃªte acteur : gain faible â†’ logits â‰ˆ 0 â†’ politique uniforme
        nn.init.orthogonal_(self.actor.weight, gain=0.01)
        # TÃªte critique : gain faible â†’ valeurs initiales proches de 0
        nn.init.orthogonal_(self.critic.weight, gain=0.01)

    def _get_cnn_out(self, k_frames: int) -> int:
        """Calcule dynamiquement la taille de sortie du CNN IMPALA."""
        dummy = torch.zeros(1, k_frames, 84, 84)
        for conv_seq in self.conv_sequences:
            dummy = conv_seq(dummy)
        dummy = torch.relu(dummy)
        return int(dummy.reshape(1, -1).shape[1])

    def forward(self, x: torch.Tensor):
        """
        x : (batch, K, 84, 84)  float32 dans [0,1]
        Retourne : logits (batch, n_actions), valeur (batch,)
        """
        # â”€â”€ Passage Ã  travers les 3 ConvSequences IMPALA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for conv_seq in self.conv_sequences:
            x = conv_seq(x)

        # â”€â”€ ReLU final + aplatissement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        x = torch.relu(x)
        x = x.reshape(x.size(0), -1)

        # â”€â”€ Couches FC + tÃªtes Actor-Critic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        z      = self.fc(x)
        logits = self.actor(z)
        value  = self.critic(z).squeeze(-1)
        return logits, value


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3.  CALCUL DES RETOURS Rt (bootstrap = 0 si Ã©pisode terminÃ©)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def compute_returns(rewards: list, dones: list, gamma: float) -> list:
    """
    Rt = rt + Î³Â·(1 - done_t)Â·R_{t+1}
    RT = 0 car l'Ã©pisode est terminÃ©.
    """
    T       = len(rewards)
    returns = [0.0] * T
    g       = 0.0
    for t in reversed(range(T)):
        g          = rewards[t] + gamma * (1.0 - dones[t]) * g
        returns[t] = g
    return returns


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4.  MISE Ã€ JOUR DU MODÃˆLE (fonction utilitaire)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MAX_BATCH_SIZE = 256  # â˜… Taille max de mini-batch pour gradient accumulation
                      #   256 pour exploiter les 24 Go VRAM du L4


def update_model(model, optimizer, states, actions, rewards, dones):
    """
    Effectue UNE mise Ã  jour A2C sur un segment de trajectoire.
    â˜… GRADIENT ACCUMULATION : forward + backward par mini-batch pour
      borner la VRAM indÃ©pendamment de la longueur de l'Ã©pisode.
      Chaque mini-batch calcule sa perte partielle, fait backward()
      (les gradients s'accumulent), puis on fait UN SEUL optimizer.step().
    Retourne un dict avec les mÃ©triques dÃ©taillÃ©es, ou None si segment trop court.
    """
    if len(rewards) < 2:
        return None

    # â”€â”€ Calcul des retours Rt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    returns_np = compute_returns(rewards, dones, GAMMA)
    returns_t  = torch.FloatTensor(returns_np).to(DEVICE)

    # â”€â”€ PrÃ©parer les tenseurs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    states_np = np.array(states)
    actions_t = torch.LongTensor(actions).to(DEVICE)

    n = len(states)
    n_chunks = (n + MAX_BATCH_SIZE - 1) // MAX_BATCH_SIZE  # nombre de mini-batches

    # â”€â”€ Passe 1 : Forward sans gradient pour calculer les avantages â”€â”€â”€â”€â”€â”€
    #   On a besoin des valeurs V(st) pour calculer les avantages AVANT
    #   de faire le vrai forward+backward. Cela ne consomme pas de VRAM
    #   car no_grad() ne stocke pas les activations intermÃ©diaires.
    with torch.no_grad():
        values_list = []
        for i in range(0, n, MAX_BATCH_SIZE):
            chunk = torch.FloatTensor(states_np[i:i+MAX_BATCH_SIZE]).to(DEVICE)
            _, values_c = model(chunk)
            values_list.append(values_c)
        values_all = torch.cat(values_list, dim=0)

    # â”€â”€ Avantage At = Rt âˆ’ V(st) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    advantages = returns_t - values_all

    # Normalisation des avantages â†’ gradients plus stables
    if advantages.numel() > 1:
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    # â”€â”€ Passe 2 : Gradient accumulation par mini-batch â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #   Chaque mini-batch fait forward â†’ loss â†’ backward().
    #   Les gradients s'ACCUMULENT dans model.parameters().grad.
    #   Ã€ la fin, on fait UN SEUL optimizer.step().
    optimizer.zero_grad()

    total_loss = 0.0
    total_actor = 0.0
    total_critic = 0.0
    total_entropy = 0.0

    for i in range(0, n, MAX_BATCH_SIZE):
        j = min(i + MAX_BATCH_SIZE, n)

        chunk_states  = torch.FloatTensor(states_np[i:j]).to(DEVICE)
        chunk_actions = actions_t[i:j]
        chunk_returns = returns_t[i:j]
        chunk_advs    = advantages[i:j]

        logits, values = model(chunk_states)

        dist      = torch.distributions.Categorical(logits=logits)
        log_probs = dist.log_prob(chunk_actions)

        # Pertes sur ce mini-batch
        L_actor   = -(log_probs * chunk_advs.detach()).mean()
        L_critic  = ((chunk_returns - values) ** 2).mean()
        L_entropy = dist.entropy().mean()

        # On divise par n_chunks pour que la somme des gradients
        # = la moyenne globale (comme si on avait fait un seul gros batch)
        chunk_loss = (L_actor + CV * L_critic - ENTROPY_COEF * L_entropy) / n_chunks
        chunk_loss.backward()

        # MÃ©triques (pour l'affichage seulement)
        total_loss    += chunk_loss.item() * n_chunks
        total_actor   += L_actor.item()
        total_critic  += L_critic.item()
        total_entropy += L_entropy.item()

    # â”€â”€ UN SEUL step d'optimisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)
    optimizer.step()

    return {
        "loss": total_loss / n_chunks,
        "L_actor": total_actor / n_chunks,
        "L_critic": total_critic / n_chunks,
        "entropy": total_entropy / n_chunks,
        "seg_len": n,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5.  BOUCLE D'ENTRAÃŽNEMENT
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def train():
    env       = gym.make(GAME)
    n_actions = env.action_space.n
    stacker   = FrameStack(K_FRAMES)

    model     = A2CNet(n_actions, K_FRAMES).to(DEVICE)
    optimizer = optim.Adam(model.parameters(), lr=LR)

    # â˜… Learning rate scheduler : dÃ©croissance linÃ©aire plus douce
    scheduler = optim.lr_scheduler.LinearLR(
        optimizer,
        start_factor=1.0,
        end_factor=0.2,       # LR finale = LR Ã— 0.2 (plus doux pour training long)
        total_iters=N_EPISODES
    )

    print(f"Actions disponibles : {n_actions}")
    print(f"Observations : {env.observation_space}")
    print(f"Mode de mise Ã  jour : {UPDATE_MODE}")
    print(model)

    episode_rewards = []
    moving_avg      = []
    window          = deque(maxlen=100)
    best_avg        = 0.0

    for ep in range(1, N_EPISODES + 1):

        # â”€â”€ RÃ©initialisation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        obs, info = env.reset()
        state     = stacker.reset(obs)

        # â˜… Fire-on-reset : Breakout nÃ©cessite l'action FIRE (1) pour lancer
        #   la balle aprÃ¨s chaque reset et aprÃ¨s chaque perte de vie.
        obs, _, terminated, truncated, info = env.step(1)  # FIRE
        if not (terminated or truncated):
            state = stacker.step(obs)

        # â˜… Life-aware : on surveille le nombre de vies pour dÃ©tecter les pertes
        lives = info.get("lives", 5)

        # Buffers pour le segment courant (entre deux pertes de vie ou Ã©pisode)
        seg_states  = []
        seg_actions = []
        seg_rewards = []
        seg_dones   = []

        ep_reward     = 0.0
        done          = False
        last_entropy  = 0.0   # DerniÃ¨re entropie mesurÃ©e (pour l'affichage)
        n_updates     = 0     # Nombre de mises Ã  jour dans cet Ã©pisode

        # â”€â”€ Collecte de la trajectoire â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        while not done:
            state_t = torch.FloatTensor(state).unsqueeze(0).to(DEVICE)

            with torch.no_grad():
                logits, _ = model(state_t)

            dist   = torch.distributions.Categorical(logits=logits)
            action = dist.sample().item()

            next_obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated

            # â˜… Life-aware : dÃ©tecter la perte de vie
            new_lives = info.get("lives", lives)
            life_lost = (new_lives < lives)
            lives     = new_lives

            # â˜… Reward clipping : borner les rÃ©compenses dans [-1, +1]
            clipped_reward = np.clip(reward, -1.0, 1.0)

            # â˜… effective_done : coupe le retour Rt Ã  chaque perte de vie
            effective_done = float(done or life_lost)

            next_state = stacker.step(next_obs)

            seg_states.append(state)
            seg_actions.append(action)
            seg_rewards.append(clipped_reward)
            seg_dones.append(effective_done)

            ep_reward += reward  # RÃ©compense BRUTE pour le suivi
            state      = next_state

            # â”€â”€ Mode LIFE : mise Ã  jour Ã  chaque perte de vie â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if UPDATE_MODE == "LIFE" and life_lost and not done:
                result = update_model(
                    model, optimizer,
                    seg_states, seg_actions, seg_rewards, seg_dones
                )
                if result is not None:
                    last_entropy = result["entropy"]
                    n_updates += 1
                # RÃ©initialiser les buffers du segment
                seg_states  = []
                seg_actions = []
                seg_rewards = []
                seg_dones   = []

            # â˜… Si l'agent a perdu une vie, appuyer sur FIRE pour relancer
            if life_lost and not done:
                obs, _, terminated, truncated, info = env.step(1)
                if not (terminated or truncated):
                    state = stacker.step(obs)
                else:
                    done = True

        # â”€â”€ Mise Ã  jour sur le segment restant (fin d'Ã©pisode) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        #   Mode EPISODE : c'est la seule update (toute la trajectoire)
        #   Mode LIFE    : update sur le dernier segment (aprÃ¨s derniÃ¨re vie)
        if len(seg_rewards) >= 2:
            result = update_model(
                model, optimizer,
                seg_states, seg_actions, seg_rewards, seg_dones
            )
            if result is not None:
                last_entropy = result["entropy"]
                n_updates += 1

        scheduler.step()  # â˜… DÃ©croissance du learning rate (1 step par Ã©pisode)

        # â”€â”€ Suivi des performances â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        episode_rewards.append(ep_reward)
        window.append(ep_reward)
        avg = np.mean(window)
        moving_avg.append(avg)

        if avg > best_avg:
            best_avg = avg
            torch.save(model.state_dict(), "a2c_breakout_best.pth")

        # â˜… Checkpoint pÃ©riodique (pour reprendre si interruption)
        if ep % SAVE_EVERY == 0:
            checkpoint = {
                "episode": ep,
                "model_state": model.state_dict(),
                "optimizer_state": optimizer.state_dict(),
                "scheduler_state": scheduler.state_dict(),
                "best_avg": best_avg,
                "episode_rewards": episode_rewards,
                "moving_avg": moving_avg,
            }
            torch.save(checkpoint, f"checkpoint_ep{ep}.pth")
            print(f"  ðŸ’¾ Checkpoint sauvegardÃ© : checkpoint_ep{ep}.pth")

        if ep % PRINT_EVERY == 0:
            current_lr = optimizer.param_groups[0]['lr']
            print(f"Ã‰pisode {ep:5d}/{N_EPISODES} | "
                  f"RÃ©comp : {ep_reward:5.1f} | "
                  f"Moy(100) : {avg:6.2f} | "
                  f"Best : {best_avg:6.2f} | "
                  f"Entropie : {last_entropy:.3f} | "
                  f"Updates : {n_updates} | "
                  f"LR : {current_lr:.2e}")

    env.close()
    return episode_rewards, moving_avg, model


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6.  VISUALISATION DES RÃ‰SULTATS
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def plot_results(episode_rewards: list, moving_avg: list):
    fig, ax = plt.subplots(figsize=(14, 6))
    ax.plot(episode_rewards, alpha=0.3, color="steelblue",
            label="RÃ©compense par Ã©pisode")
    ax.plot(moving_avg, color="darkorange", linewidth=2,
            label="Moyenne glissante (100 Ã©pisodes)")
    ax.set_xlabel("Ã‰pisode")
    ax.set_ylabel("RÃ©compense totale")
    ax.set_title(f"A2C Convolutionnel â€“ {GAME.split('/')[-1]} (amÃ©liorÃ©)")
    ax.legend()
    ax.grid(alpha=0.3)
    plt.tight_layout()
    plt.savefig("a2c_rewards.png", dpi=150)
    print("Figure sauvegardÃ©e sous 'a2c_rewards.png'")
    plt.show()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# POINT D'ENTRÃ‰E
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    episode_rewards, moving_avg, model = train()
    plot_results(episode_rewards, moving_avg)

    torch.save(model.state_dict(), "a2c_breakout_final.pth")
    print("ModÃ¨le final sauvegardÃ© sous 'a2c_breakout_final.pth'")
    print("Meilleur modÃ¨le sauvegardÃ© sous 'a2c_breakout_best.pth'")
